@book{koller_friedman,
 author = {Koller, D. and Friedman, N.},
 title = {Probabilistic Graphical Models: Principles and Techniques},
 year = {2009},
 isbn = {0262013193, 9780262013192},
 publisher = {The MIT Press},
} 

@article{weihs2019artificial,
  title={Artificial Agents Learn Flexible Visual Representations by Playing a Hiding Game},
  author={Weihs, Luca and Kembhavi, Aniruddha and Han, Winson and Herrasti, Alvaro and Kolve, Eric and Schwenk, Dustin and Mottaghi, Roozbeh and Farhadi, Ali},
  journal={arXiv preprint arXiv:1912.08195},
  year={2019}
}


@inproceedings{chen2020visual,
  title={Visual hide and seek},
  author={Chen, Boyuan and Song, Shuran and Lipson, Hod and Vondrick, Carl},
  booktitle={Artificial Life Conference Proceedings},
  pages={645--655},
  year={2020},
  organization={MIT Press}
}


@article{faraji2018balancing,
  title={Balancing new against old information: the role of puzzlement surprise in learning},
  author={Faraji, Mohammadjavad and Preuschoff, Kerstin and Gerstner, Wulfram},
  journal={Neural computation},
  volume={30},
  number={1},
  pages={34--83},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{hazan2019provably,
  title={Provably Efficient Maximum Entropy Exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@article{DBLP:journals/corr/abs-1804-06424,
  author    = {Glen Berseth and
               Xue Bin Peng and
               Michiel van de Panne},
  title     = {Terrain {RL} Simulator},
  journal   = {CoRR},
  volume    = {abs/1804.06424},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.06424},
  archivePrefix = {arXiv},
  eprint    = {1804.06424},
  timestamp = {Mon, 13 Aug 2018 16:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-06424.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lee2019efficient,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}

@article{tschantz2020reinforcement,
  title={Reinforcement Learning through Active Inference},
  author={Tschantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},
  journal={arXiv preprint arXiv:2002.12636},
  year={2020}
}

@article{annabi2020autonomous,
  title={Autonomous learning and chaining of motor primitives using the Free Energy Principle},
  author={Annabi, Louis and Pitti, Alexandre and Quoy, Mathias},
  journal={arXiv preprint arXiv:2005.05151},
  year={2020}
}


@article{DBLP:journals/corr/AchiamS17,
  author    = {Joshua Achiam and
               Shankar Sastry},
  title     = {Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1703.01732},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01732},
  archivePrefix = {arXiv},
  eprint    = {1703.01732},
  timestamp = {Mon, 13 Aug 2018 16:46:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AchiamS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tschantz2020scaling,
  title={Scaling active inference},
  author={Tschantz, Alexander and Baltieri, Manuel and Seth, Anil K and Buckley, Christopher L},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}



@article{friston2016active,
  title={Active inference and learning},
  author={Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni and others},
  journal={Neuroscience \& Biobehavioral Reviews},
  volume={68},
  pages={862--879},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{gae,
title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
author = {J. Schulman and P. Moritz and S. Levine and M. Jordan and P. Abbeel},
booktitle = {International Conference on Learning Representations (ICLR)},
year  = 2016
}

@inproceedings{kim2019curiosity,
  title={Curiosity-Bottleneck: Exploration by Distilling Task-Specific Novelty},
  author={Kim, Youngjin and Nam, Wontae and Kim, Hyunwoo and Kim, Ji-Hoon and Kim, Gunhee},
  booktitle={International Conference on Machine Learning},
  pages={3379--3388},
  year={2019}
}

@article{kim2020active,
  title={Active World Model Learning with Progress Curiosity},
  author={Kim, Kuno and Sano, Megumi and De Freitas, Julian and Haber, Nick and Yamins, Daniel},
  journal={arXiv preprint arXiv:2007.07853},
  year={2020}
}


@article{shyam2018model,
  title={Model-based active exploration},
  author={Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  journal={arXiv preprint arXiv:1810.12162},
  year={2018}
}

@inproceedings{lopes2012exploration,
  title={Exploration in model-based reinforcement learning by empirically estimating learning progress},
  author={Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-Yves},
  booktitle={Advances in neural information processing systems},
  pages={206--214},
  year={2012}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}

@article{still2012information,
  title={An information-theoretic approach to curiosity-driven reinforcement learning},
  author={Still, Susanne and Precup, Doina},
  journal={Theory in Biosciences},
  volume={131},
  number={3},
  pages={139--148},
  year={2012},
  publisher={Springer}
}

@inproceedings{sun2011planning,
  title={Planning to be surprised: Optimal bayesian exploration in dynamic environments},
  author={Sun, Yi and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Artificial General Intelligence},
  pages={41--51},
  year={2011},
  organization={Springer}
}

@inproceedings{aytar2018playing,
  title={Playing hard exploration games by watching youtube},
  author={Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Thomas and Wang, Ziyu and de Freitas, Nando},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2930--2941},
  year={2018}
}

@article{baker2019emergent,
  title={Emergent Tool Use From Multi-Agent Autocurricula},
  author={Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  journal={arXiv preprint arXiv:1909.07528},
  year={2019}
}

@article{guo2019efficient,
  title={Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy},
  author={Guo, Yijie and Choi, Jongwook and Moczulski, Marcin and Bengio, Samy and Norouzi, Mohammad and Lee, Honglak},
  journal={arXiv preprint arXiv:1907.10247},
  year={2019}
}

@article{leestate,
  title={STATE MARGINAL MATCHING WITH MIXTURES OF POLICIES},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Salakhutdinov, Ruslan and Levine, Sergey}
}

@article{gangwani2018learning,
  title={Learning self-imitating diverse policies},
  author={Gangwani, Tanmay and Liu, Qiang and Peng, Jian},
  journal={arXiv preprint arXiv:1805.10309},
  year={2018}
}

@article{edwards2018imitating,
  title={Imitating latent policies from observation},
  author={Edwards, Ashley D and Sahni, Himanshu and Schroecker, Yannick and Isbell, Charles L},
  journal={arXiv preprint arXiv:1805.07914},
  year={2018}
}

@inproceedings{schmidhuber1991curious,
  title={Curious model-building control systems},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. international joint conference on neural networks},
  pages={1458--1463},
  year={1991}
}

@article{torabi2018generative,
  title={Generative adversarial imitation from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1807.06158},
  year={2018}
}

@inproceedings{liu2018imitation,
  title={Imitation from observation: Learning to imitate behaviors from raw video via context translation},
  author={Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1118--1125},
  year={2018},
  organization={IEEE}
}

@article{achiam2017surprise,
  title={Surprise-based intrinsic motivation for deep reinforcement learning},
  author={Achiam, Joshua and Sastry, Shankar},
  journal={arXiv preprint arXiv:1703.01732},
  year={2017}
}

@article{torabi2018behavioral,
  title={Behavioral cloning from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1805.01954},
  year={2018}
}

@article{oh2018self,
  title={Self-imitation learning},
  author={Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  journal={arXiv preprint arXiv:1806.05635},
  year={2018}
}

@article{oudeyer2009intrinsic,
  title={What is intrinsic motivation? A typology of computational approaches},
  author={Oudeyer, Pierre-Yves and Kaplan, Frederic},
  journal={Frontiers in neurorobotics},
  volume={1},
  pages={6},
  year={2009},
  publisher={Frontiers}
}

@article{oudeyer2007intrinsic,
  title={Intrinsic motivation systems for autonomous mental development},
  author={Oudeyer, Pierre-Yves and Kaplan, Frdric and Hafner, Verena V},
  journal={IEEE transactions on evolutionary computation},
  volume={11},
  number={2},
  pages={265--286},
  year={2007},
  publisher={IEEE}
}

@inproceedings{toussaint06,
 author = {Toussaint, M. and Storkey, A.},
 title = {Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2006},
} 

@article{schneider1994life,
  title={Life as a manifestation of the second law of thermodynamics},
  author={Schneider, Eric D and Kay, James J},
  journal={Mathematical and computer modelling},
  volume={19},
  number={6-8},
  pages={25--48},
  year={1994},
  publisher={Elsevier}
}

@book{schrodinger1944life,
  title={What is life? The physical aspect of the living cell and mind},
  author={Schr{\"o}dinger, Erwin},
  year={1944},
  publisher={Cambridge University Press Cambridge}
}

@article{boltzmann1886second,
  title={The second law of thermodynamics},
  author={Boltzmann, Ludwig},
  year={1886}
}

@article{friston2009free,
  title={The free-energy principle: a rough guide to the brain?},
  author={Friston, Karl},
  journal={Trends in cognitive sciences},
  volume={13},
  number={7},
  pages={293--301},
  year={2009},
  publisher={Elsevier}
}

@INPROCEEDINGS{attias,
    author = {H. Attias},
    title = {Planning by Probabilistic Inference},
    booktitle = {Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics},
    year = {2003}
}

@InProceedings{ebrl,
  title = 	 {Actor-Critic Reinforcement Learning with Energy-Based Policies},
  author = 	 {N. Heess and D. Silver and Y. W. Teh},
  booktitle = 	 {European Workshop on Reinforcement Learning (EWRL)},
  year = 	 {2013},
}

@inproceedings{ep,
 author = {Minka, T. P.},
 title = {Expectation Propagation for Approximate Bayesian Inference},
 booktitle = {Uncertainty in Artificial Intelligence (UAI)},
 year = {2001},
}

@inproceedings{mpo,
title = {Maximum a Posteriori Policy Optimisation},
author = {A. Abdolmaleki and J. T. Springenberg and Y. Tassa and R. Munos and N. Heess and M. Riedmiller},
booktitle = {International Conference on Learning Representations (ICLR)},
year  = 2018
}

@article{williams,
 author = {Williams, R. J.},
 title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
 journal = {Machine Learning},
 issue_date = {May 1992},
 volume = {8},
 number = {3-4},
 month = may,
 year = {1992},
 pages = {229--256}
} 

@article{WilliamsPeng91,
  author = {Williams, R. J. and Peng, J.},
  journal = {Connection Science},
  number = 3,
  pages = {241-268},
  title = {Function optimization using connectionist reinforcement learning
	algorithms},
  volume = 3,
  year = 1991
}

@INPROCEEDINGS{suttonadp,
    author = {R. S. Sutton},
    title = {Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {1990},
}

@inproceedings{pgq,
author = {O'Donoghue, B. and Munos, R. and Kavukcuoglu, K. and Mnih, V.},
year = {2017},
title = {PGQ: Combining policy gradient and Q-learning},
booktitle = {International Conference on Learning Representations (ICLR)}
}

@article{sallans,
 author = {Sallans, B. and Hinton, G. E.},
 title = {Reinforcement Learning with Factored States and Actions},
 journal = {Journal of Machine Learning Research},
 volume = {5},
 month = dec,
 year = {2004}
 },

@article{KLMSurvey,
    author = "L. P. Kaelbling and M. L. Littman and A. P. Moore",
    title = "Reinforcement Learning: A Survey",
    journal = "Journal of Artificial Intelligence Research",
    volume = "4",
    pages = "237-285",
    year = "1996",
url={http://people.csail.mit.edu/lpk/papers/rl-survey.ps}}

@inproceedings{todorov_kalman_duality, 
author={E. Todorov}, 
booktitle={Conference on Decision and Control (CDC)},
title={General duality between optimal control and estimation}, 
year={2008},
}

@inproceedings{covariant,
author = {J. A. Bagnell and J. Schneider},
title = {Covariant Policy Search},
booktitle = {International Joint Conference on Artifical Intelligence (IJCAI)},
year = {2003},
}

@InProceedings{reps,
  author =       "Peters, J. and  M{\"u}lling, K. and Alt{\"u}n, Y.",
  title =        "Relative Entropy Policy Search",
  booktitle =    "AAAI Conference on Artificial Intelligence (AAAI)",
  year =         "2010",
}

@inproceedings{mfgps,
title = {Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics},
author = {Levine, S. and Abbeel, P.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2014},
}

@inproceedings{lmdp_policy,
  author    = {E. Todorov},
  title     = {Policy gradients in linearly-solvable MDPs},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year      = {2010},
}

@inproceedings{ldmp_irl,
 author = {Dvijotham, K. and Todorov, E.},
 title = {Inverse Optimal Control with Linearly-solvable MDPs},
 booktitle = {International Conference on International Conference on Machine Learning (ICML)},
 year = {2010},
} 

@inproceedings{pi2,
  title = {Learning Policy Improvements with Path Integrals},
  author = {Theodorou, E. A. and Buchli, J. and Schaal, S.},
  booktitle = {International Conference on  Artificial Intelligence and Statistics (AISTATS 2010)},
  year = {2010},
}

@InProceedings{trpo,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {J. Schulman and S. Levine and P. Moritz and M. I. Jordan and P. Abbeel},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2015},
}

@phdthesis{levine_thesis,
author = {Levine, S.},
title = {Motor skill learning with local trajectory methods},
school = {Stanford University},
year = {2014},
}

@phdthesis{ziebart_thesis,
author = {Ziebart, B.},
title = {Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
school = {Carnegie Mellon University},
year = {2010},
}

@article{kappen_oc,
  author    = {H. J. Kappen},
  title     = {Optimal control theory and the linear bellman equation},
  journal   = {Inference and Learning in Dynamic Models},
  year      = {2011},
  pages     = {363-387},
}

@article{kappen_pgm,
  author    = {H. J. Kappen and
               V. G{\'o}mez and
               M. Opper},
  title     = {Optimal control as a graphical model inference problem},
  journal   = {Machine Learning},
  volume    = {87},
  number    = {2},
  year      = {2012},
  pages     = {159-182},
}

@inproceedings{rawlik_soc,
  author    = {K. Rawlik and
               M. Toussaint and
               S. Vijayakumar},
  title     = {On Stochastic Optimal Control and Reinforcement Learning
               by Approximate Inference},
  year = {2013},
  booktitle = {Robotics: Science and Systems (RSS)},
}

@inproceedings{toussaint_soc,
  author    = {M. Toussaint},
  title     = {Robot trajectory optimization using approximate inference},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2009},
}

@inproceedings{toussaint_pgm,
  title={Hierarchical POMDP Controller Optimization by Likelihood Maximization},
  author={Toussaint, M. and Charlin, L. and Poupart, P.},
  booktitle={Uncertainty in Artificial Intelligence (UAI)},
  volume={24},
  pages={562--570},
  year={2008}
}

@article{kalman_filter,
  author={R. Kalman},
  title={A new approach to linear filtering and prediction problems},
  journal={ASME Transactions journal of basic engineering},
  volume={82},
  number={1},
  pages={35-45},
  year={1960}
}

@inproceedings{todorov_lmdp,
  author    = {E. Todorov},
  title     = {Linearly-solvable Markov decision problems},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2006},
}

@inproceedings{rwr,
 author = {Peters, J. and Schaal, S.},
 title = {Reinforcement Learning by Reward-weighted Regression for Operational Space Control},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2007},
} 


@inproceedings{neumann,
  author    = {G. Neumann},
  title     = {Variational Inference for Policy Search in changing situations},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2011},
}

@inproceedings{levine_vgps,
  author    = {S. Levine and V. Koltun},
  title     = {Variational policy search via trajectory optimization},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2013},
}

@inproceedings{em_policy_search,
  title = {Efficient Sample Reuse in EM-Based Policy Search},
  author = {Hachiya, H. and Peters, J. and Sugiyama, M.},
  booktitle = {European Conference on Machine Learning (ECML)},
  year = {2009},
}

@article{vmp,
    title={Variational message passing},
    author={Winn, J. and Bishop, C.},
    journal={Journal of Machine Learning Research},
    volume={6},
    year={2005},
    pages={661-694}
}

@inproceedings{haarnoja,
  title={Reinforcement Learning with Deep Energy-Based Policies},
  author={Haarnoja, T. and Tang, H. and Abbeel, P. and Levine, S.},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@inproceedings{sac,
title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
author  = {T. Haarnoja and A. Zhou and P. Abbeel and S. Levine},
year  = {2018},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1801.01290.pdf}
}

@inproceedings{pcl,
title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
author  = {O. Nachum and M. Norouzi and K. Xu and D. Schuurmans},
year  = {2017},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1702.08892.pdf}
}

@inproceedings{gps,
 author = {Levine, S. and Koltun, V.},
 title = {Guided Policy Search},
 booktitle = {International Conference on International Conference on Machine Learning (ICML)},
 year = {2013},
} 

@inproceedings{maxentirl,
 author = {Ziebart, B. D. and Maas, A. and Bagnell, J. A. and Dey, A. K.},
 title = {Maximum Entropy Inverse Reinforcement Learning},
 booktitle = {International Conference on Artificial Intelligence (AAAI)},
 year = {2008},
} 

@inproceedings{haarnoja_composable,
author = {Haarnjoa, T. and Pong, V. and Zhou, A. and Dalal, M. and Abbeel, P. and Levine, S.},
title = {Composable Deep Reinforcement Learning for Robotic Manipulation},
booktitle = {International Conference on Robotics and Automation (ICRA)},
year = {2018},
}

@article{trust_pcl,
  author    = {O. Nachum and
               M. Norouzi and
               K. Xu and
               D. Schuurmans},
  title     = {Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
  journal   = {CoRR},
  volume    = {abs/1707.01891},
  year      = {2017},
}

@inproceedings{gpirl,
 author = {Levine, S. and Popovi\'{c}, Z. and Koltun, V.},
 title = {Nonlinear Inverse Reinforcement Learning with Gaussian Processes},
 booktitle = {Neural Information Processing Systems (NIPS)},
 year = {2011},
}

@inproceedings{localioc,
    author = {S. Levine and V. Koltun},
    title = {Continuous Inverse Optimal Control with Locally Optimal Examples},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2012},
}

@inproceedings{legibility,
  author    = {A. D. Dragan and
               K. C. T. Lee and
               S. S. Srinivasa},
  title     = {Legibility and predictability of robot motion},
  booktitle = {International Conference on Human-Robot Interaction (HRI)},
  year      = {2013},
}

@inproceedings{maxentdeepirl,
author = {M. Wulfmeier and
P. Ondruska and
I. Posner},
title = {Maximum Entropy Deep Inverse Reinforcement Learning},
Booktitle = {Neural Information Processing Systems Conference, Deep Reinforcement Learning Workshop},
year = {2015},
}

@inproceedings{maxcausalent,
  author    = {B. D. Ziebart and
               J. A. Bagnell and
               A. K. Dey},
  title     = {Modeling Interaction via the Principle of Maximum Causal Entropy},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2010},
}

@inproceedings{kitani1,
  author    = {D. Huang and
               K. M. Kitani},
  title     = {Action-Reaction: Forecasting the Dynamics of Human Interaction},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2014},
}

@inproceedings{kitani2,
  author    = {D. Huang and
               A. Farahmand and
               K. M. Kitani and
               J. A. Bagnell},
  title     = {Approximate {MaxEnt} Inverse Optimal Control and Its Application for
               Mental Simulation of Human Interactions},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  year      = {2015},
}

@article{realnvp,
title={Latent Space Policies for Hierarchical Reinforcement Learning},
author={T. Haarnoja and K. Hartikainen and P. Abbeel and S. Levine},
journal   = {CoRR},
volume    = {abs/1804.02808},
year      = {2018},
}

@inproceedings{Javdani, 
    AUTHOR    = {S. Javdani and S. Srinivasa and J. A. Bagnell}, 
    TITLE     = {Shared Autonomy via Hindsight Optimization}, 
    BOOKTITLE = {Robotics: Science and Systems (RSS)}, 
    YEAR      = {2015}, 
}

@inproceedings{airl,
title={Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
author={Fu, J. and Luo, K. and Levine, S.},
}

@inproceedings{hausman,
title={Learning an Embedding Space for Transferable Robot Skills},
author={K. Hausman and J. T. Springenberg and Z. Wang and N. Heess and M. Riedmiller},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
}

@article{learning_to_explore,
  author    = {A. Gupta and
               R. Mendonca and
               Y. Liu and
               P. Abbeel and
               S. Levine},
  title     = {Meta-Reinforcement Learning of Structured Exploration Strategies},
  journal   = {CoRR},
  volume    = {abs/1802.07245},
  year      = {2018},
}

@article{endtoend,
 author = {Levine, S. and Finn, C. and Darrell, T. and Abbeel, P.},
 title = {End-to-end Training of Deep Visuomotor Policies},
 journal = {Journal of Machine Learning Research},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
} 

@inproceedings{cgps,
    author = {Sergey Levine and Vladlen Koltun},
    title = {Learning Complex Neural Network Policies with Trajectory Optimization},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2014},
}
@inproceedings{schulman,
title = {Equivalence Between Policy Gradients and Soft Q-Learning},
author  = {J. Schulman and X. Chen and P. Abbeel},
year  = {2017},
booktitle = {arXiv},
URL = {https://arxiv.org/pdf/1704.06440}
}

@inproceedings{nvil,
  author    = {A. Mnih and
               K. Gregor},
  title     = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2014}
}

@inproceedings{gcl,
  author    = {Chelsea Finn and
               Sergey Levine and
               Pieter Abbeel},
  title     = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization},
    booktitle   = {International Conference on Machine Learning (ICML)},
  year      = {2016},
}

@inproceedings{gan,
title = {Generative Adversarial Nets},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2014},
}

@inproceedings{gail,
title = {Generative Adversarial Imitation Learning},
author = {Ho, J. and Ermon, S.},
booktitle = {Neural Information Processing Systems (NIPS)},
year = {2016},
}

@article{finn_adversarial,
  author    = {Chelsea Finn and
               Paul Christiano and
               Pieter Abbeel and
               Sergey Levine},
  title     = {A Connection between Generative Adversarial Networks, Inverse Reinforcement
               Learning, and Energy-Based Models},
  journal   = {CoRR},
  volume    = {abs/1611.03852},
  year      = {2016},
}

@inproceedings{bornschein2016bidirectional,
  title={Bidirectional helmholtz machines},
  author={Bornschein, Jorg and Shabanian, Samira and Fischer, Asja and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={2511--2519},
  year={2016}
}

@article{marino2018iterative,
  title={Iterative amortized inference},
  author={Marino, Joseph and Yue, Yisong and Mandt, Stephan},
  journal={arXiv preprint arXiv:1807.09356},
  year={2018}
}

@inproceedings{rasmus2015semi,
  title={Semi-supervised learning with ladder networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  booktitle={Advances in neural information processing systems},
  pages={3546--3554},
  year={2015}
}

@inproceedings{sonderby2016ladder,
  title={Ladder variational autoencoders},
  author={S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  booktitle={Advances in neural information processing systems},
  pages={3738--3746},
  year={2016}
}

@inproceedings{zhao2017learning,
  title={Learning hierarchical features from deep generative models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={4091--4099},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{hsu2017unsupervised,
  title={Unsupervised learning of disentangled and interpretable representations from sequential data},
  author={Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  booktitle={Advances in neural information processing systems},
  pages={1878--1889},
  year={2017}
}


@article{Houthooft2016,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1605.09674},
file = {:home/gberseth/Dropbox/Research/Papers/MachineLearning/Reinforecement Learning/VIME Variational Information Maximizing Exploration.pdf:pdf},
title = {{VIME: Variational Information Maximizing Exploration}},
url = {http://arxiv.org/abs/1605.09674},
year = {2016}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}

@article{Burda2018,
abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/},
archivePrefix = {arXiv},
arxivId = {1808.04355},
author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
eprint = {1808.04355},
file = {:home/gberseth/Dropbox/Research/Papers/MachineLearning/Reinforecement Learning/Large-Scale Study of Curiosity-Driven Learning.pdf:pdf},
title = {{Large-Scale Study of Curiosity-Driven Learning}},
url = {http://arxiv.org/abs/1808.04355},
year = {2018}
}
@article{Pathak2017,
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
file = {:home/gberseth/Dropbox/Research/Papers/MachineLearning/Reinforecement Learning/Curiosity-driven Exploration by Self-supervised Prediction.pdf:pdf},
title = {{Curiosity-driven Exploration by Self-supervised Prediction}},
year = {2017}
}

@article{Pathak2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.04161v1},
author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
eprint = {arXiv:1906.04161v1},
file = {:home/gberseth/Dropbox/Research/Papers/MachineLearning/Reinforecement Learning/Self-Supervised Exploration via Disagreement.pdf:pdf},
title = {{Self-Supervised Exploration via Disagreement}},
year = {2019}
}
@article{Taiga2019,
abstract = {This paper provides an empirical evaluation of recently developed exploration algorithms within the Arcade Learning Environment (ALE). We study the use of different reward bonuses that incentives exploration in reinforcement learning. We do so by fixing the learning algorithm used and focusing only on the impact of the different exploration bonuses in the agent's performance. We use Rainbow, the state-of-the-art algorithm for value-based agents, and focus on some of the bonuses proposed in the last few years. We consider the impact these algorithms have on performance within the popular game MON-TEZUMA'S REVENGE which has gathered a lot of interest from the exploration community, across the the set of seven games identified by Bellemare et al. (2016) as challenging for exploration, and easier games where exploration is not an issue. We find that, in our setting, recently developed bonuses do not provide significantly improved performance on MONTEZUMA'S REVENGE or hard exploration games. We also find that existing bonus-based methods may negatively impact performance on games in which exploration is not an issue and may even perform worse than-greedy exploration.},
archivePrefix = {arXiv},
arxivId = {arXiv:1908.02388v1},
author = {Ta{\"{i}}ga, Adrien Ali and Fedus, William and Machado, Marlos C and Courville, Aaron and Bellemare, Marc G},
eprint = {arXiv:1908.02388v1},
file = {:home/gberseth/Dropbox/Research/Papers/MachineLearning/Reinforecement Learning/Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment.pdf:pdf},
title = {{Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment}},
year = {2019}
}

@article{10.1371/journal.pone.0006421,
    author = {Friston, Karl J. AND Daunizeau, Jean AND Kiebel, Stefan J.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Reinforcement Learning or Active Inference?},
    year = {2009},
    month = {07},
    volume = {4},
    url = {https://doi.org/10.1371/journal.pone.0006421},
    pages = {1-13},
    abstract = {This paper questions the need for reinforcement learning or control theory when optimising behaviour. We show that it is fairly simple to teach an agent complicated and adaptive behaviours using a free-energy formulation of perception. In this formulation, agents adjust their internal states and sampling of the environment to minimize their free-energy. Such agents learn causal structure in the environment and sample it in an adaptive and self-supervised fashion. This results in behavioural policies that reproduce those optimised by reinforcement learning and dynamic programming. Critically, we do not need to invoke the notion of reward, value or utility. We illustrate these points by solving a benchmark problem in dynamic programming; namely the mountain-car problem, using active perception or inference under the free-energy principle. The ensuing proof-of-concept may be important because the free-energy formulation furnishes a unified account of both action and perception and may speak to a reappraisal of the role of dopamine in the brain.},
    number = {7},
    doi = {10.1371/journal.pone.0006421}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}


@inproceedings{klyubin2005empowerment,
  title={Empowerment: A universal agent-centric measure of control},
  author={Klyubin, Alexander S and Polani, Daniel and Nehaniv, Chrystopher L},
  booktitle={2005 IEEE Congress on Evolutionary Computation},
  volume={1},
  pages={128--135},
  year={2005},
  organization={IEEE}
}

@incollection{NIPS2015_5668,
title = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
author = {Mohamed, Shakir and Jimenez Rezende, Danilo},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2125--2133},
year = {2015},
publisher = {Curran Associates, Inc.}
}

@InProceedings{10.1007/11553090_75,
author="Klyubin, Alexander S.
and Polani, Daniel
and Nehaniv, Chrystopher L.",
editor="Capcarr{\`e}re, Mathieu S.
and Freitas, Alex A.
and Bentley, Peter J.
and Johnson, Colin G.
and Timmis, Jon",
title="All Else Being Equal Be Empowered",
booktitle="Advances in Artificial Life",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="744--753",
abstract="The classical approach to using utility functions suffers from the drawback of having to design and tweak the functions on a case by case basis. Inspired by examples from the animal kingdom, social sciences and games we propose empowerment, a rather universal function, defined as the information-theoretic capacity of an agent's actuation channel. The concept applies to any sensorimotoric apparatus. Empowerment as a measure reflects the properties of the apparatus as long as they are observable due to the coupling of sensors and actuators via the environment.",
isbn="978-3-540-31816-3"
}

@article{argall2009survey,
  title={A survey of robot learning from demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and autonomous systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={Elsevier}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{ale,
 author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
 title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
 booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
 series = {IJCAI'15},
 year = {2015},
 isbn = {978-1-57735-738-4},
 location = {Buenos Aires, Argentina},
 pages = {4148--4152},
 numpages = {5},
 url = {http://dl.acm.org/citation.cfm?id=2832747.2832830},
 acmid = {2832830},
 publisher = {AAAI Press},
} 

@article{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  journal={arXiv preprint arXiv:1606.01540},
}

@article{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{lehman2011abandoning,
  title={Abandoning objectives: Evolution through the search for novelty alone},
  author={Lehman, Joel and Stanley, Kenneth O},
  journal={Evolutionary computation},
  volume={19},
  number={2},
  pages={189--223},
  year={2011},
  publisher={MIT Press}
}

@article{bansal2017emergent,
  title={Emergent complexity via multi-agent competition},
  author={Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  journal={arXiv preprint arXiv:1710.03748},
  year={2017}
}

@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{sukhbaatar2017intrinsic,
  title={Intrinsic motivation and automatic curricula via asymmetric self-play},
  author={Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1703.05407},
  year={2017}
}

@inproceedings{chentanez2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},
  booktitle={Advances in neural information processing systems},
  pages={1281--1288},
  year={2005}
}

@article{ueltzhoffer2018deep,
	title={Deep active inference},
	author={Ueltzh{\"o}ffer, Kai},
	journal={Biological Cybernetics},
	volume={112},
	number={6},
	pages={547--573},
	year={2018},
	publisher={Springer}
}

@inproceedings{kempka2016vizdoom,
  title={Vizdoom: A doom-based ai research platform for visual reinforcement learning},
  author={Kempka, Micha{\l} and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Ja{\'s}kowski, Wojciech},
  booktitle={2016 IEEE Conference on Computational Intelligence and Games (CIG)},
  pages={1--8},
  year={2016},
  organization={IEEE}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={ICLR},
  year={2014}
}

@article{burda2018rnd,
    title = {Exploration by Random Network Distillation},
    author = {Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
    year = {2018},
    journal={ICLR},
}

@inproceedings{Biehl2018FreeE,
  title={Free energy , empowerment , and predictive information compared},
  author={M. Biehl and C. Guckelsberger and C. Salge and S. Smith and D. Polani},
  year={2018}
}